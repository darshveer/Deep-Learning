# Learning
- My time at Valuebound was from 13th of May to the 14th of July, 2025 under an amazing mentor - Mr. Tanay. It was an internship about mostly Machine Learning and Deep Learning and I learnt a lot from it.
- I learnt a lot from this internship, ranging from Neural Networks to Transformers to Attention Mechanisms.
- We started off with the basics of machine learning such as RNNs and then went on to the more advanced models such as the Transformer model and GPT-3. These were covered in the first and second week, along with learning about each of the GPT models and their details.
- I tried building my own models for text generation, and they worked decently well, as in they did genrate text. It's just that none of the text they generated made any sense.
- In the third and fourth weeks we covered more in-detail about machine learning with transformers and MCTS, diving into detail about the AlphaZero and AlphaGo architectures. Further learning included the paper "How do Language Models Learn Facts" - which mentioned about the way models learn based on the input given, and new knowledge emerges only after the plateau of existing knowledge is gone.
- I then implemented my own verion of AlphaZero, but for TicTacToe, and it worked quite well. After this was done, it was my time to strat wokring on something fo my own.
- The next 3 weeks were dedicated to working on the company's ongoing prject - NxtHyre (I was responsible to design the user flows for various parts of the website, along with Akshy bhaiya, in Figma), along with understadning and working on some real-life probllem that could be solved through AI/ML.
- I finally decided on the problem (after much ponder) I wanted to solve and it was something I had seen multiple people struggled with - the problem of progressive glasses, or even glasses in general.
- The solution I proposed initially required a lot of hardware support. The idea was to  use cameras on the outside of a parit of spectacles which would capture what the user was looking at. This was done by using retina sensors on the inside of the glass, which would capture the retina's movemenets and its size to know exactly where the user was looking, and how much light they had in their room.
- These measurmented will then be processed by and AI model and then sent to a elctrolytic lens (in particular the Fresnel Liquid Crystal Lens), which would then change its power based on the amount of voltage applied to it. This would help the user see what they were looking at, at different distances without needing to strain their eyes when changing distances.
- However, as this was a bit of a stretch as the materials needed to be sourced and would take some time to build, we fell back onto the software solution. Basically, the idea was for someone with power to be able to see what was on their screen without the need for spectacles.
- The idea was to take the prescription of the user as input, and then use some kind of weiner deconvolution to "de-blur" the image before the user sees it. In this way, when the user sees the image, the de-blurred image gets blurred again, basically recreating the original image for the user.
- There were multiple issues with this. The first issue was that even though the contrast of the recreated image for the user would be terrible, because the blurring would create halos around the existing image's pixels, thereby reducing the contrast.
- The second issue was that the image we were recreating was still blurry. Even though we were simulating the deconvolution, I, being a spectacle user, was still not able to see the full image completely clearly.
- The third issue was the even if we could recreate the image on software, it still won;t be perfect, because each pixel reaches our retinal at different angles, and thus, the final image will be blurry, unless we implement somehthing called a pinhole display. This is used for directing the light rays into our eyes, and thus, the images will look sharper.
- I initially started by using normal Gaussian deblur, which did not produce suitable results. Apparently, using only formulae was not working because there were too many factors to consider before deconvoluting.
- Thus, not onlt did I shift to a more reliable deconvolution method, called Wiener deconvolution, but I also trained a model which would take images, use the deconv on it, and then simulate the blur. It would then learn the difference between the two, and then apply the loss to the wiener deconvolution to correct the deconvolution.
- This did produce better results, but to get even better results, the compute I had was not sufficient.
- Lastly, after I was done with all of this, I leanr that Tanay bhaiya also had a certification in CyberSec, and so I immediately pivoted as I am deeply interested in the same.
- I was given a few resources to learn from, and I learnt about the History of Gryptography from "The Code Book" by Simon Singh. I then downloaded wireshark and learnt about it features. Sadly, as this was the last week, I couldn't get much done (especially with me falling sick every other day).
- All in all, this was an amazing summer internship and I learnt a lot from Tanay bhaiya and everyone else. I would say the main skills I learnt apart from the technical aspects were:
    - Product Management
    - Problem Solving
    - Teamwork
    - Communication
